---
path: "/weekly-journals/week-3-goal-analysis"
date: "2017-09-18"
title: 'Weekly Journal 3: Ch 3 Goal Analysis. Articles: Patricia Smith "learner anaylsis" and "context analysis"'
---

## Summary

### Analyzing the Learner Context

What do instructional designers and Sherlock Holmes have in common?

They are clever investigators. They examine and deconstruct individuals, their environment, and context. They contemplate possibilities and perceive the need for a solution or goal before investing time and energy.

They do it with front-end analysis.

While a detective like Holmes uses analysis absolutely, many instructional designers struggle with it. It may be perceived as a wasted effort, in favor of rushing a project out to door to please a client or phantom business need.

For designers, it is in their best interest to engage with front-end analysis and prove that’s it worth the time and energy and identify the clearest goal. After all, it’s elementary.

Instructional designers can deduce solutions with three components:
- Instructional context
- Prospective learners
- Learning tasks

These can be analyzed by observing the learning context, the learner’s needs and their environment.

### The Learning Context
The first step of front-end analysis is analyzing the learning context. Context not only includes physical areas, but also the temporal and social. It’s wherever the learning takes place. In fact, experts consider contextual analysis to absolutely central in instructional design. Learning context involves two components:
- Evidence of a need for instruction
- Learning environment's description

### Determine Instructional Needs
Designers conduct needs assessment to determine whether instruction should be designed. The purpose of needs assessment is to determine if there is an actual need for new instruction. The designer, at this point, decides to spend energy on new training.

There are different models to conduct needs analyses:
* Using the **Problem Model**: This model is used is address a problem identified in the organization's mission.
    * The Problem Model determines:
        * If there is a problem
        * The cause of the problem
        * Whether the solution to the problem is learning
        * Whether instruction for these learning goals is currently offered
* Using the **Innovation Model**: Examines changes or innovation in the organization and whether new goals should be accommodated.
    * The Innovation Mode determines:
        * The nature of innovation or change
        * Learning goals that accompany this innovation
        * If goals are appropriate
        * Begins the next phase of instruction.
* Using the **Discrepancy Model**: This model presumes that learning goals are already identified and instruction is currently related to the goal.
    * The Discrepancy Model:
        * List the goals of the instructional system
        * How well identified goals are already achieved
        * Determines gaps between what is and what should be
        * Prioritizes gaps according to a criteria
        * Determines which gaps are instructional needs and appropriate for design

### Analyzing the Learner Environment
In order for designers to make the most of instruction, they need spend time analyzing the learners environment and leverage possible resources. An instructional designer should procure learning about:
* Learners (discussed in depth below)
* Teachers and trainers
* Existing curricula
* Physical equipment
* Facilities
* Organizational tendencies

An instructional designer can be a thought leader when it comes to how training fits into the grander system. Considering how one piece of training can be used in other parts of instruction can save time and energy.

Using a subject matter expert for information can help designers get what they need.

### Analyzing the Learners
Part of needs assessment is the subject of instructional design: the learner.

Without an accurate representation of a **target audience or population**, an instructional designer cannot properly form a goal for learning.

Learner characteristics are an important element of instructional design. It’s nothing without learners. The four major categories of learner characteristics are:
* Cognitive
* Physiological
* Affective
* Social

A designer may not or need to consider all of these characteristics for all learning all audiences. It depends on the type of characteristics that are more apparent in the context of the learner.

For example if the training was to be in a hospital or Health Institute physiological characteristics like sensory information, general health, age information would be more critical to the design process so how does a designer assess learning characteristics interview they can have target audience surveys or examine job descriptions of personal profiles they also do some research for articles about age groups

The author’s focus is primarily cognitive, with dimensions such as:
* Learner similarities versus differences
* Stable or Changing characteristics

### Specific Prior Learning
The most important factor for a designer to consider is the audience is specific prior learning. What do the learners already know in the area they will be learning?

It's important for a designer to investigate if the learner has background knowledge or skills and a current test they're asking them to learn is important. Of particular interest is the variation in the background between perspective learners.

### Conducting a Goal Analysis
Knowing what a designer knows about a learner, they can begin goal analysis. The main point of goal analysis is to provide an unambiguous description of what the learner will be doing when performing the goal.

At this point in instructional analysis, a designer:
* Classifies the goal into a domain of learning
* Identifies and sequences the major steps required to perform the goal

There are four learning domains that determine the scope and sequence:
* Verbal Information
* Intellectual Skills
* Psychomotor skills
* Attitudes

**Verbal Information** requires learners to provide specific responses to specific questions. For example, a learner is asked to recite the major muscle groups in the body.

Since verbal information doesn’t involve problem-solving or making decisions, the best sequence for analyzing verbal goals is chronological. If not, it should be based on inherent relationships.

**Intellectual Skills** require the learner to do some unique cognitive activity. There are four common types of intellectual skills:
* Making discriminations
* Forming concepts
* Applying rules
* Solving problems

Some examples of intellectual skills include solving a math problem (well-defined problem) or how to engage in conflict management (ill-defined problem).

Intellectual skill goals can be analyzed using step-by-step descriptions. The learner is taught how to make decisions that work towards a goal.

**Psychomotor Skills** requires the learner to coordinate of mental and physical activity. For example, the learner demonstrating how to separate an egg from a yolk, using the egg shell as a tool.

Like intellectual skill goals, psychomotor skills can be analyzed using step-by-step descriptions. The learner is taught how to make decisions that work towards a goal.

Lastly, **Attitudes** require the learner to choose to do something. For example, a pilot choosing safe flying while operating a small plane.

Attitudes can use procedural flowcharts if the behavioral outcome is psychomotor or intellectual. Either way, it’s necessary to identify the behavior exhibited when the desired attitude is met.

Through learner, context and goal analysis, a designer should have a clear understanding of how to get the learner from status quo to a desired state.

## Critique

The approaches suggested by the authors, as with most approaches mentioned in this course to date, offer considerable value if time is not a consideration. In most fast paced business environments where training is often developed just-in-time, rigorous needs assessment, analysis, and proper goal setting and objectives fall by the wayside. There is a need, and it gets filled. we think this is why adoption of ADDIE is so prevalent. It’s accessible, you can use what you need, when you need it, and rigor may or may not apply.

One critique we have is on performance problems/discrepancies and determining whether or not existing instruction is being delivered efficiently. Smith discusses how, “certain performance problems may be better soles by support rather than training,” through an Electronic Performance Support System rather than teaching the person new skills, and giving a person advice, and tools to guide a person through a task.

We respectfully disagree with Smith. If a learner is not performing well, not performing quickly enough, we think it’s unfair to totally rule out any type of instruction and jump immediately to an EPSS. While an EPSS may be cheaper than additional training on a surface level, it may be worth including time management training. Because it’s possible the performing of a task not getting done is not related to whether or not a learner can perform a task, but rather finding the time to complete it, and do it correctly.

Should a company offer time management training it is likely a person will not only become better in a given task, it’s likely they may perform better in other tasks as well. Especially should an employee transfer elsewhere in the company, they will have a new soft skill that will benefit them outside the previous task. This is why we think it would be important to address the learner in a performance case, and determine whether or not time management training would be beneficial to them, and also why the needs assessment is so critical: a task not getting done all the way may not directly be an instructional problem of the task, but an instructional problem that simply requires _different_ instruction.

Patricia Smith speaks to many influences of stable and changing states for a person. we agree with the point of view. But, how does one possibly account for all of these elements?

Of noteworthy mention, we agree with the statement about cognitive learning styles. Evidence against learning styles has stuck around since the 1980’s, yet, for some reason perpetuates in our education. According to some research, the reason it is popular to identify with a learning style is because it’s still though of actual science. While many have researched this possibility, it appears little evidence suggests that people learn best that way. Two main reasons:
* That type of learning is self-prescribed about the learner. This is supported by Smith. There is no scientific evidence that suggests instructional design benefits from learners doing this.
* The other we found, from _Make It Stick_, is that learning styles mean that learners are comfortable with that style. Therefore, learning any other way isn’t challenging them at all. In other terms, the natural encoding process from working memory to long-term memory isn’t forming strong associations.
It appears the authors are in line with this thinking. Everyone has a specific learning style, but it’s not as simple as auditory or hands-on. There are many dimensions to learning.

We totally agree with Smith about intelligence testing.  There have definitely been abuses of things like IQ, but we also believe they have provided value. Smith frames the abuses of IQ around the ‘“good brains” conceptions of IQ and blanket judgments made and communicated to students...’ [61] My own experience with IQ testing was a benefit to me I think.  I was tested when I was in the 2nd grade and found to have sufficient IQ to join a special class that would help us focus on problem solving and critical thinking.  I along with 8 of my classmates were able to be instructed in a way that worked better for us.  It kept us from being bored in class and helped me to stretch my intellect in a better environment.  We believe that perhaps IQ should be considered more in certain circumstances as long as the “good brains” and other similar notions aren’t employed.

## Self-reflection

Interestingly, we am currently in the process of conducting a Problem Model needs assessment at work, although we was not aware it was called the “Problem Model” until encountering this week’s texts. My team has been asked to examine “what went wrong” with a system-level tool (online form) called, the Value Summary Improvement Portal.

The original goal of the Portal was to bring greater transparency to improvement work conducted throughout the system by providing a platform that allowed teams to view, share and report projects from throughout the system. Need was based on mass duplication of efforts and no ability to spread improvements that worked in one area to another.

To date, the Portal has not delivered on its intended goal. People are indeed inputting the work, but the work itself is of “poor quality.” There are proven strategies and methods for process improvement and the form, supposedly, had been designed to guide the user through them as they went. The output we have found mirrors Smith’s assertion that, “Frequently, poor achievement or performance has more than one cause.”

To this end, we have found:

1. **“They’re using it, but they’re using it wrong.”** (SME quote)
The original design, of note, did not include the people who actually do the work (health care professionals), nor did it include instructional designers or technologists with UI/UX expertise. This resulted in questions, training, and guidance laden with jargon and business terminology foreign to most health care professionals. <u>How can we can’t expect them to “do it right” if they can’t understand it in the first place?</u>

2. We have a culture of solution generating, not problem solving.
There is a larger cultural barrier in place. Traditionally, health professionals have been rewarded for quick thinking and acting = solution generating. The introduction of process improvement methodology runs very counter to deep-seated attitudes and rewards for fast-acting. Not process-driven (often slow) problem solving. <u>How can we facilitate cultural adoption of process improvement?</u>

The original intent of the tool was not misguided, and high utilization has prompted us to redesign rather than retire the tool. We are now in the process of reorganizing the development team to better meet the audience's needs. Greater emphasis on clarity and ease of use with structured training only in topic areas where we have data supporting knowledge or performance gaps. Instructional designers in close collaboration with health care professionals will develop learning materials, and there is heavy consideration of UI/UX in the technical redesign. Culturally, we have spent the past five years developing a platform dedicated to fostering improvement culture called, Accelerate. Data to date is promising, but we have a long way to go in achieving meaningful systems outcomes.

Playing on the issue of time management training, it’s something has quickly become one of the objectives in our project. Nurses know how to document giving blood, but they often don’t have the time. However, those not charting are often new grad nurses who do not know how to manage their time between caring for patients (top priority) and charting/documenting care and services. While we can beat the horse to death on _how_ to chart, the reality is that nurses already know how to do it, and no EPSS will help them. Because they need to find the time to do it. What the nurses could really benefit from is time management training, and how to balance caring for patients and “doing paperwork.” Because, due to their moral development as nurses, it’s likely they are going to take care of their patients over charting information or charging patients for their care. Especially on a unit where they know patients may not be able to afford the care they are receiving (e.g. blood administration), so give time constraints and an attitudinal problem. It feels off to call their compassion for patients and likelihood to not charge for a service a problem, but that just goes to show: 1. They need time management, 2. They need training on the finance fundamentals of nonprofits and 3. Just how important it is to be charting, and charting correctly

Another thing I’ve noticed is in data gathering, it’s often a disaster in large organizations to gather data from “many sources.” In such a large organizations, many different groups track and define tasks in many different ways, for different reasons. When we went to get data from a few different groups, I’m getting a lot of conflicting data, which has resulting in deviating from the initial problem, to now digging into why and how all the data can be different. While we think it’s important to get data from a variety of sources, it’s led me down a rabbit hole, and I’m wasting a lot of time trying to deal with new surfacing problems we didn’t intend to.

I’ve reflected on how bad my organization is when it comes to task analysis or any kind of mental stimulation when it comes to learning about learners. This chapter helped me see why. We simply do not set goals around the learner.

For example, we are about to start a major initiative geared around increasing product usage. We know there is a correlation between usage and renewals. So far, the plan is to get a user catalogue up for our users to view and apply to their projects. After reading this chapter, I’ve wondered if we even know thats what users want or need! What it may turn into is a bunch of wasted time or effort because we will have to change it again anyway.

Learning about learners is similar to marketing. For a product or service to do well, it needs to have people who see value in the training. Not only that, they will turn down alternatives to choose your offering.

It’s the same thing with instructional design. To know if a learning will substantiate, we have to understand my market, the context. Only then can we enter the market. we think the analogy breaks down with creating a market. we don’t have evidence of breaking into a new market with learning. The one thing we did find the introduction of design thinking into ADDIE that looks promising. But even then, the main proponent of design thinking is empathy with people.

One thing that we reflected on a bit this week was something that struck me from Dick and Carey’s book.  “The SME approach tends to stress _knowing_, whereas the instructional design approach stresses _doing_.” [42]  I’m in what seems to be an odd situation at work where I’m developing some training for other software engineers about making accessible software.  On this subject I’m a SME, but I’m also wearing the instructional designer hat in this particular circumstance.  I’m finding that we need to keep balance between dumping content on the learners and really designing the instruction appropriately.  It’s harder than we would have thought it would be.
